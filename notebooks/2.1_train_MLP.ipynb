{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA 0: NVIDIA GeForce GTX 1650 Ti\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Add parent directory into system path\n",
    "import sys, os\n",
    "sys.path.insert(1, os.path.abspath(os.path.normpath('..')))\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.init import calculate_gain\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f'CUDA {i}: {torch.cuda.get_device_name(i)}')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = 'cpu'\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.MLP import Davies2021\n",
    "net = Davies2021(N_layers=8, width=32, activation=nn.Softplus(30), last_activation=nn.Softplus(30)).to(device)\n",
    "#net = Davies2021(N_layers=8, width=28, activation=nn.SiLU(), last_activation=nn.Identity()).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pyigl_import] module igl not found. trying to import pyigl\n",
      "ImplicitDataset (1000000 points)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from utils.dataset_generator import ImplicitDataset\n",
    "\n",
    "dataset_name = '../datasets/box_1f0_gyroid_4pi'\n",
    "output_stl = dataset_name+'.stl'\n",
    "train_dataset = ImplicitDataset.from_file(file=dataset_name+'_train.npz', device=device)\n",
    "\n",
    "# # filter out discontinous points\n",
    "# grads = train_dataset.grads\n",
    "# norm_grad = torch.linalg.norm(grads, dim=1)\n",
    "# mark = torch.logical_and(norm_grad > 0.9, norm_grad < 1.001)\n",
    "# train_dataset.grads = grads[mark]\n",
    "# train_dataset.points = train_dataset.points[mark]\n",
    "# train_dataset.sdfs = train_dataset.sdfs[mark]\n",
    "\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.callback_scheduler import CallbackScheduler\n",
    "\n",
    "# Optimization\n",
    "## ADA\n",
    "#torch.nn.utils.clip_grad_norm_(net.parameters(), 10.0)\n",
    "optimizer=torch.optim.Adam(net.parameters(), lr=0.005, betas=(0.9, 0.999), eps=1e-6, amsgrad=False)\n",
    "lr_scheduler = CallbackScheduler([\n",
    "    CallbackScheduler.nothing(),\n",
    "    CallbackScheduler.reduce_lr(0.2),\n",
    "    CallbackScheduler.nothing(),\n",
    "    CallbackScheduler.reduce_lr(0.5),\n",
    "    CallbackScheduler.nothing(),\n",
    "], optimizer=optimizer, model=net, eps=1e-7, patience=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max epoch: 3000 (PRINT: 300, NUM_BATCH: 100)\n",
      "#0 Loss: 0.000543 na na na na na \n",
      "#300 Loss: 0.013910 na na na na na \n",
      "[callback scheduler]:  do_nothing\n",
      "#600 Loss: 0.013910 na na na na na \n",
      "#900 Loss: 0.013910 na na na na na \n",
      "[callback scheduler]:  reduce_learning_rate\n",
      "#1200 Loss: 0.019019 na na na na na \n",
      "[callback scheduler]:  do_nothing\n",
      "#1500 Loss: 0.019019 na na na na na \n",
      "#1800 Loss: 0.019019 na na na na na \n",
      "[callback scheduler]:  reduce_learning_rate\n",
      "#2100 Loss: 0.018067 na na na na na \n",
      "#2400 Loss: 0.018067 na na na na na \n",
      "[callback scheduler]:  do_nothing\n",
      "#2700 Loss: 0.018067 na na na na na \n",
      "#3000 Loss: 0.018067 na na na na na \n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "from utils.dataset_generator import batch_loader\n",
    "from math import ceil\n",
    "\n",
    "batch_size = 10000\n",
    "lr_step = 500\n",
    "\n",
    "NUM_BATCH = int(ceil(len(train_dataset.points) / batch_size))\n",
    "MAX_EPOCHS = int(lr_step * (len(lr_scheduler)+1))\n",
    "PRINT_EVERY_EPOCH = int(MAX_EPOCHS // 10)\n",
    "NUM_TRAIN_SAMPLES = len(train_dataset)\n",
    "\n",
    "print(f'Max epoch: {MAX_EPOCHS} (PRINT: {PRINT_EVERY_EPOCH}, NUM_BATCH: {NUM_BATCH})')\n",
    "\n",
    "try:\n",
    "    epoch = 0\n",
    "    while epoch < MAX_EPOCHS:\n",
    "        #loss = torch.empty((1,), device=device, requires_grad=True)\n",
    "        for points, sdfs in batch_loader(train_dataset.points, train_dataset.sdfs, batch_size=batch_size):\n",
    "            \n",
    "            lr_scheduler.optimizer.zero_grad()\n",
    "            loss = net.loss(points, sdfs)\n",
    "            #loss = loss + points.shape[0] * loss\n",
    "\n",
    "            #loss = loss / NUM_TRAIN_SAMPLES\n",
    "            loss.backward()\n",
    "            lr_scheduler.optimizer.step(lambda: loss)\n",
    "        \n",
    "        lr_scheduler.step_when((epoch % lr_step) == lr_step - 1, verbose=True)\n",
    "        \n",
    "        if epoch % PRINT_EVERY_EPOCH == 0:\n",
    "            print(f'#{epoch} {net.print_loss()}')\n",
    "        \n",
    "        epoch += 1\n",
    "    print(f'#{epoch} {net.print_loss()}\\nSuccess')\n",
    "except KeyboardInterrupt as e:\n",
    "    print('Bye bye')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(net.state_dict(), f'{dataset_name}-MLP.pth')\n",
    "net.load_state_dict(torch.load(f'{dataset_name}-MLP.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.visualizer import SDFVisualize\n",
    "net.to(device)\n",
    "visualize = SDFVisualize(z_level=0.0, scale_offset=0.3, nums=200, device=device)\n",
    "visualize.from_nn(net, bounds_from_mesh=output_stl)\n",
    "visualize.from_mesh(output_stl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset_generator import TestDataset, run_batch\n",
    "dataset_name = '../datasets/box_1f0_gyroid_4pi'\n",
    "test_dataset = TestDataset(dataset_name+'_test.npz', device=device)\n",
    "kwarg = {'reducer': torch.mean, 'batch_size': 10000}\n",
    "#print('Train residual: ', net.test_residual(train_dataset.points).cpu().detach().numpy())\n",
    "print('Test uniform SDFS: ', run_batch(net.test, test_dataset.uniform.points, test_dataset.uniform.sdfs, **kwarg).cpu().detach().numpy())\n",
    "print('Test uniform residual:', run_batch(net.test_residual, test_dataset.uniform.points, **kwarg).cpu().detach().numpy())\n",
    "print('Test uniform norm grads: ', run_batch(net.test_norm_gradient, test_dataset.uniform.points, test_dataset.uniform.norm_grads, **kwarg).cpu().detach().numpy())\n",
    "print('Test random SDFS: ', run_batch(net.test, test_dataset.random.points, test_dataset.random.sdfs, **kwarg).cpu().detach().numpy())\n",
    "print('Test random residual: ', run_batch(net.test_residual, test_dataset.random.points, **kwarg).cpu().detach().numpy())\n",
    "#print('Actual uniform residual: ', torch.mean((test_dataset.uniform.gradients - 1)**2).cpu().detach().numpy())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "338d088b7365852483cbb7f745c4a9fbf18ad887a6ae80fcf58992043df108c7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('sdf': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
