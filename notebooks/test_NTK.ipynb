{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA 0: NVIDIA GeForce GTX 1650 Ti\n",
      "ImplicitDataset (64000 points)\n",
      "TestDataset (1000000 points, 1000000 points, 1000000 points)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Add parent directory into system path\n",
    "import sys, os\n",
    "sys.path.insert(1, os.path.abspath(os.path.normpath('..')))\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.init import calculate_gain\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f'CUDA {i}: {torch.cuda.get_device_name(i)}')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device('cpu')\n",
    "\n",
    "from utils.operator import gradient\n",
    "from models import M4, MLP_PINN\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "torch.set_printoptions(threshold=10_000)\n",
    "torch.set_printoptions(linewidth=200)\n",
    "torch.set_printoptions(edgeitems=3)\n",
    "\n",
    "import os\n",
    "from utils.dataset_generator import ImplicitDataset, TestDataset, batch_loader\n",
    "\n",
    "dataset_name = '../datasets/box_1f0_gyroid_4pi'\n",
    "output_stl = dataset_name+'.stl'\n",
    "train_dataset = ImplicitDataset.from_file(file=dataset_name+'_train.npz', device=device)\n",
    "test_dataset = TestDataset(dataset_name+'_test.npz', device=device)\n",
    "print(train_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_SDF(model, x, sdfs):\n",
    "    y = model(x)\n",
    "    return torch.mean((y - sdfs)**2)\n",
    "\n",
    "def loss_PDE(model, x):\n",
    "    x.requires_grad_(True)\n",
    "    y = model(x)\n",
    "    norm_grad = gradient(y, x).norm(dim=1)\n",
    "    return torch.mean((norm_grad - 1)**2)\n",
    "\n",
    "def loss_grad(model, x, grad):\n",
    "    x.requires_grad_(True)\n",
    "    y = model(x)\n",
    "    p = gradient(y, x)\n",
    "    return torch.mean((p - grad).norm(dim=1))\n",
    "\n",
    "def loss_cosine_similarity(model, x, grad):\n",
    "    x.requires_grad_(True)\n",
    "    y = model(x)\n",
    "    p = gradient(y, x)\n",
    "    norm_p = torch.linalg.norm(p, dim=1)\n",
    "    norm_g = torch.linalg.norm(grad, dim=1)\n",
    "    return torch.mean(-torch.einsum('ij,ij->i', p, grad)/norm_p/norm_g)\n",
    "\n",
    "def loss_second_order_gradient(model, x):\n",
    "    x.requires_grad_(True)\n",
    "    y = model(x)\n",
    "    grad = gradient(y, x)\n",
    "    norm_grad = grad.norm(dim=1)\n",
    "    grad_grad = gradient(grad, x)\n",
    "    return torch.mean(grad_grad ** 2)\n",
    "\n",
    "def loss_relu_residual(model, x):\n",
    "    x.requires_grad_(True)\n",
    "    y = model(x)\n",
    "    p = gradient(y, x)\n",
    "    norm_p = torch.linalg.norm(p, dim=1)\n",
    "    return torch.nn.ReLU()(norm_p - 1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 1000\n",
      "0: 0.00014489727618638426\n",
      "1: 1282.265625\n",
      "2: 1282.2659912109375\n",
      "3: 1282.7261962890625\n",
      "4: 1282.265869140625\n",
      "5: 1282.2657470703125\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# Test PINN\n",
    "import numpy as np\n",
    "from utils.jacobian import extend, JacobianMode\n",
    "\n",
    "_points = train_dataset.points.clone()\n",
    "_sdfs = train_dataset.sdfs.clone()\n",
    "_grads = train_dataset.grads.clone()\n",
    "\n",
    "net = MLP_PINN(N_layers=8, width=32, activation=nn.Softplus(30), last_activation=nn.Softplus(30)).to(device)\n",
    "\n",
    "extend(net, (3,))\n",
    "\n",
    "batch_sizes = [1000]\n",
    "K_eigvals = [[], [], [], [], [], []]\n",
    "    \n",
    "for bs in batch_sizes:\n",
    "    print(f'Batch size: {bs}')\n",
    "    for points, sdfs, grads in batch_loader(_points, _sdfs, _grads, batch_size=bs):\n",
    "        net.zero_grad()\n",
    "        with JacobianMode(net):\n",
    "            _loss = loss_PDE(net, points)\n",
    "            _loss.backward()\n",
    "            jac = net.jacobian()\n",
    "            K = jac @ jac.T\n",
    "            K_eigvals[1].append(torch.linalg.eigvals(K).abs().sum())\n",
    "\n",
    "        net.zero_grad()\n",
    "        with JacobianMode(net):\n",
    "            _loss = loss_grad(net, points, grads)\n",
    "            _loss.backward()\n",
    "            jac = net.jacobian()\n",
    "            K = jac @ jac.T\n",
    "            K_eigvals[2].append(torch.linalg.eigvals(K).abs().sum())\n",
    "\n",
    "        net.zero_grad()\n",
    "        with JacobianMode(net):\n",
    "            _loss = loss_cosine_similarity(net, points, grads)\n",
    "            _loss.backward()\n",
    "            jac = net.jacobian()\n",
    "            K = jac @ jac.T\n",
    "            K_eigvals[3].append(torch.linalg.eigvals(K).abs().sum())\n",
    "\n",
    "        net.zero_grad()\n",
    "        with JacobianMode(net):\n",
    "            _loss = loss_second_order_gradient(net, points)\n",
    "            _loss.backward()\n",
    "            jac = net.jacobian()\n",
    "            K = jac @ jac.T\n",
    "            K_eigvals[4].append(torch.linalg.eigvals(K).abs().sum())\n",
    "\n",
    "        net.zero_grad()\n",
    "        with JacobianMode(net):\n",
    "            _loss = loss_relu_residual(net, points)\n",
    "            _loss.backward()\n",
    "            jac = net.jacobian()\n",
    "            K = jac @ jac.T\n",
    "            K_eigvals[5].append(torch.linalg.eigvals(K).abs().sum())\n",
    "\n",
    "        net.zero_grad()\n",
    "        with JacobianMode(net):\n",
    "            _loss = loss_SDF(net, points, sdfs)\n",
    "            _loss.backward()\n",
    "            jac = net.jacobian()\n",
    "            K = jac @ jac.T\n",
    "            K_eigvals[0].append(torch.linalg.eigvals(K).abs().sum())\n",
    "\n",
    "        break\n",
    "\n",
    "    for i,vals in enumerate(K_eigvals):\n",
    "        print(f'{i}: {torch.mean(torch.tensor(vals))}')\n",
    "\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 1000\n",
      "0: 0.001178080914542079\n",
      "1: 0.001699618762359023\n",
      "2: 0.0006427342887036502\n",
      "3: 0.03387424722313881\n",
      "4: 0.002335398457944393\n",
      "5: 0.0\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# Test M4\n",
    "import numpy as np\n",
    "from models import M4\n",
    "\n",
    "_points = train_dataset.points.clone()\n",
    "_sdfs = train_dataset.sdfs.clone()\n",
    "_grads = train_dataset.grads.clone()\n",
    "\n",
    "net = M4(N_layers=8, width=32, activation=nn.Softplus(30), last_activation=nn.Softplus(30)).to(device)\n",
    "# summary(\n",
    "#     net, (1,3), depth=3, verbose=2, col_width=16,\n",
    "#     row_settings=[\"var_names\"], \n",
    "#     col_names=[\"kernel_size\", \"input_size\", \"output_size\", \"num_params\"]\n",
    "# )\n",
    "\n",
    "extend(net, (3,))\n",
    "\n",
    "batch_sizes = [1000]\n",
    "K_eigvals = [[], [], [], [], [], []]\n",
    "    \n",
    "for bs in batch_sizes:\n",
    "    print(f'Batch size: {bs}')\n",
    "    for points, sdfs, grads in batch_loader(_points, _sdfs, _grads, batch_size=bs):\n",
    "\n",
    "        \n",
    "\n",
    "        net.zero_grad()\n",
    "        with JacobianMode(net):\n",
    "            _loss = loss_PDE(net, points)\n",
    "            _loss.backward()\n",
    "            jac = net.jacobian()\n",
    "            K = jac @ jac.T\n",
    "            K_eigvals[1].append(torch.linalg.eigvals(K).abs().sum())\n",
    "\n",
    "        net.zero_grad()\n",
    "        with JacobianMode(net):\n",
    "            _loss = loss_grad(net, points, grads)\n",
    "            _loss.backward()\n",
    "            jac = net.jacobian()\n",
    "            K = jac @ jac.T\n",
    "            K_eigvals[2].append(torch.linalg.eigvals(K).abs().sum())\n",
    "\n",
    "        net.zero_grad()\n",
    "        with JacobianMode(net):\n",
    "            _loss = loss_cosine_similarity(net, points, grads)\n",
    "            _loss.backward()\n",
    "            jac = net.jacobian()\n",
    "            K = jac @ jac.T\n",
    "            K_eigvals[3].append(torch.linalg.eigvals(K).abs().sum())\n",
    "\n",
    "        net.zero_grad()\n",
    "        with JacobianMode(net):\n",
    "            _loss = loss_second_order_gradient(net, points)\n",
    "            _loss.backward()\n",
    "            jac = net.jacobian()\n",
    "            K = jac @ jac.T\n",
    "            K_eigvals[4].append(torch.linalg.eigvals(K).abs().sum())\n",
    "\n",
    "        net.zero_grad()\n",
    "        with JacobianMode(net):\n",
    "            _loss = loss_relu_residual(net, points)\n",
    "            _loss.backward()\n",
    "            jac = net.jacobian()\n",
    "            K = jac @ jac.T\n",
    "            K_eigvals[5].append(torch.linalg.eigvals(K).abs().sum())\n",
    "\n",
    "        net.zero_grad()\n",
    "        with JacobianMode(net):\n",
    "            _loss = loss_SDF(net, points, sdfs)\n",
    "            _loss.backward()\n",
    "            jac = net.jacobian()\n",
    "            K = jac @ jac.T\n",
    "            K_eigvals[0].append(torch.linalg.eigvals(K).abs().sum())\n",
    "\n",
    "    for i,vals in enumerate(K_eigvals):\n",
    "        print(f'{i}: {torch.mean(torch.tensor(vals))}')\n",
    "\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.9999991059303284\n",
      "1: 0.6931437849998474\n",
      "2: 1.8329178094863892\n",
      "3: 0.03477806970477104\n",
      "4: 0.5044451355934143\n",
      "5: 1178081.0\n"
     ]
    }
   ],
   "source": [
    "zero_lambda = torch.mean(torch.tensor(K_eigvals[0]))\n",
    "for i,vals in enumerate(K_eigvals):\n",
    "        print(f'{i}: {zero_lambda/(torch.mean(torch.tensor(vals))+1e-9)}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "338d088b7365852483cbb7f745c4a9fbf18ad887a6ae80fcf58992043df108c7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('sdf': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
